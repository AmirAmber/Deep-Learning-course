# -*- coding: utf-8 -*-
"""LeNet5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qhIi_b1NT2AJw9VaWpv6e1y_Y7C174z_
"""

#@title LeNet5 Initializations
!apt-get install -y texlive-latex-extra texlive-fonts-recommended dvipng cm-super

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

#Use fancy font
plt.rc('text',usetex=True)
plt.rcParams.update({
    'font.family': 'serif',
    'font.serif': 'cm'})

# HyperParameters
batch_size = 256
num_classes = 10
learning_rate = 0.001
num_epochs = 25

# Run on GPU if applicable
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load and preprocess the data
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

trainset = torchvision.datasets.FashionMNIST(root='./data/', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

testset = torchvision.datasets.FashionMNIST(root='./data/', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

#@title LeNet5 without any regularization

#Defining the convolutional neural network
class LeNet5(nn.Module):
    def __init__(self, num_classes):
        super(LeNet5, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.fc1 = nn.Linear(1600, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.fc2(out)
        return out

model = LeNet5(num_classes).to(device)

#Setting the loss function
cost_metric = nn.CrossEntropyLoss()

#Setting the optimizer with the model parameters and learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

#this is defined to print how many steps are remaining when training
total_step = len(trainloader)


#Train the model
LeNet5_train_accuracies = []
LeNet5_test_accuracies = []

for epoch in range(num_epochs):
    model.train()
    for i, (images, labels) in enumerate(trainloader):
        images = images.to(device)
        labels = labels.to(device)

        #Forward pass
        outputs = model(images)
        loss = cost_metric(outputs, labels)

        #Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        """if (i+1) % 100 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))
        """
    #Test the model
    model.eval()
    with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in trainloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      train_accuracy = round(100 * correct / total , 2)
      LeNet5_train_accuracies.append(train_accuracy)
      print('Accuracy of the network on the train images: {} %'.format(train_accuracy))

      for images, labels in testloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      test_accuracy = round(100 * correct / total, 2)
      LeNet5_test_accuracies.append(test_accuracy)
      print('Accuracy of the network on the test images: {} %'.format(test_accuracy))

#@title Plot of train and test accuracies for the LeNet5 model without any regularization
epochs = range(1, num_epochs + 1)
plt.figure(figsize=(8, 5))
plt.plot(epochs, LeNet5_train_accuracies, label="Train Accuracy", marker='o')
plt.plot(epochs, LeNet5_test_accuracies, label="Test Accuracy", marker='x')
plt.title("LeNet5 - no regularization: Accuracy as a function of Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy [\%]")
plt.legend()
plt.grid()
plt.show()

#@title LeNet5 with Dropout
#Defining the convolutional neural network
class LeNet5_Dropout(nn.Module):
    def __init__(self, num_classes):
        super(LeNet5_Dropout, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.dropout = nn.Dropout(0.4)
        self.fc1 = nn.Linear(1600, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.dropout(out)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.fc2(out)
        return out

model = LeNet5_Dropout(num_classes).to(device)

#Setting the loss function
cost_metric = nn.CrossEntropyLoss()

#Setting the optimizer with the model parameters and learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

#this is defined to print how many steps are remaining when training
total_step = len(trainloader)

#Train the model
LeNet5_DO_train_accuracies = []
LeNet5_DO_test_accuracies = []

for epoch in range(num_epochs):
    model.train()
    for i, (images, labels) in enumerate(trainloader):
        images = images.to(device)
        labels = labels.to(device)

        #Forward pass
        outputs = model(images)
        loss = cost_metric(outputs, labels)

        #Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        """ if (i+1) % 100 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))
"""
    #Test the model
    model.eval()
    with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in trainloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      train_accuracy = round(100 * correct / total , 2)
      LeNet5_DO_train_accuracies.append(train_accuracy)
      print('Accuracy of the network on the train images: {} %'.format(train_accuracy))

      for images, labels in testloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      test_accuracy = round(100 * correct / total, 2)
      LeNet5_DO_test_accuracies.append(test_accuracy)
      print('Accuracy of the network on the test images: {} %'.format(test_accuracy))

#@title Plot of train and test accuracies for the LeNet5 model with Dropout
epochs = range(1, num_epochs + 1)
plt.figure(figsize=(8, 5))
plt.plot(epochs, LeNet5_DO_train_accuracies, label="Train Accuracy", marker='o')
plt.plot(epochs, LeNet5_DO_test_accuracies, label="Test Accuracy", marker='x')
plt.title("LeNet5 - with Dropout: Accuracy as a function of Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy [\%]")
plt.legend()
plt.grid()
plt.show()

#@title LeNet5 with Weight Decay
#Defining the convolutional neural network
class LeNet5_WD(nn.Module):
    def __init__(self, num_classes):
        super(LeNet5_WD, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.fc1 = nn.Linear(1600, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.fc2(out)
        return out

model = LeNet5_WD(num_classes).to(device)

#Setting the loss function
cost_metric = nn.CrossEntropyLoss()

#Setting the optimizer with the model parameters and learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)

#this is defined to print how many steps are remaining when training
total_step = len(trainloader)


#Train the model
LeNet5_WD_train_accuracies = []
LeNet5_WD_test_accuracies = []

for epoch in range(num_epochs):
    model.train()
    for i, (images, labels) in enumerate(trainloader):
        images = images.to(device)
        labels = labels.to(device)

        #Forward pass
        outputs = model(images)
        loss = cost_metric(outputs, labels)

        #Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        """if (i+1) % 100 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))
"""
    #Test the model
    model.eval()
    with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in trainloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      train_accuracy = round(100 * correct / total , 2)
      LeNet5_WD_train_accuracies.append(train_accuracy)
      print('Accuracy of the network on the train images: {} %'.format(train_accuracy))

      for images, labels in testloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      test_accuracy = round(100 * correct / total, 2)
      LeNet5_WD_test_accuracies.append(test_accuracy)
      print('Accuracy of the network on the test images: {} %'.format(test_accuracy))

#@title Plot of train and test accuracies for the LeNet5 model with Weight Decay
epochs = range(1, num_epochs + 1)
plt.figure(figsize=(8, 5))
plt.plot(epochs, LeNet5_WD_train_accuracies, label="Train Accuracy", marker='o')
plt.plot(epochs, LeNet5_WD_test_accuracies, label="Test Accuracy", marker='x')
plt.title("LeNet5 - with with Weight Decay: Accuracy as a function of Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy [\%]")
plt.legend()
plt.grid()
plt.show()

#@title LeNet5 with Batch Normalization
#Defining the convolutional neural network
class LeNet5_BN(nn.Module):
    def __init__(self, num_classes):
        super(LeNet5_BN, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.fc1 = nn.Linear(1600, 64)
        self.bn1 = nn.BatchNorm1d(64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc1(out)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.fc2(out)
        return out

model = LeNet5_BN(num_classes).to(device)

#Setting the loss function
cost_metric = nn.CrossEntropyLoss()

#Setting the optimizer with the model parameters and learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

#this is defined to print how many steps are remaining when training
total_step = len(trainloader)


#Train the model
LeNet5_BN_train_accuracies = []
LeNet5_BN_test_accuracies = []

for epoch in range(num_epochs):
    model.train()
    for i, (images, labels) in enumerate(trainloader):
        images = images.to(device)
        labels = labels.to(device)

        #Forward pass
        outputs = model(images)
        loss = cost_metric(outputs, labels)

        #Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        """if (i+1) % 100 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))
"""
    #Test the model
    model.eval()
    with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in trainloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      train_accuracy = round(100 * correct / total , 2)
      LeNet5_BN_train_accuracies.append(train_accuracy)
      print('Accuracy of the network on the train images: {} %'.format(train_accuracy))

      for images, labels in testloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      test_accuracy = round(100 * correct / total, 2)
      LeNet5_BN_test_accuracies.append(test_accuracy)
      print('Accuracy of the network on the test images: {} %'.format(test_accuracy))

#@title Plot of train and test accuracies for the LeNet5 model with Batch Normalization
epochs = range(1, num_epochs + 1)
plt.figure(figsize=(8, 5))
plt.plot(epochs, LeNet5_BN_train_accuracies, label="Train Accuracy", marker='o')
plt.plot(epochs, LeNet5_BN_test_accuracies, label="Test Accuracy", marker='x')
plt.title("LeNet5 - with with Batch Normalization: Accuracy as a function of Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy [\%]")
plt.legend()
plt.grid()
plt.show()

#@title Final results summary
models = ['LeNet5 without Regularization', 'LeNet5 with Dropout', 'LeNet5 with L2 Weight Decay', 'LeNet5 with Batch Normalization']
final_train_accuracies = [LeNet5_train_accuracies[-1], LeNet5_DO_train_accuracies[-1],
                          LeNet5_WD_train_accuracies[-1], LeNet5_BN_train_accuracies[-1]]
final_test_accuracies = [LeNet5_test_accuracies[-1], LeNet5_DO_test_accuracies[-1],
                         LeNet5_WD_test_accuracies[-1], LeNet5_BN_test_accuracies[-1]]

# Data for the table
data = []
for model, train_acc, test_acc in zip(models, final_train_accuracies, final_test_accuracies):
    data.append([model, f"{train_acc:.2f}%", f"{test_acc:.2f}%"])

# Add column headers
columns = ["Model", "Final Train Accuracy [\%]", "Final Test Accuracy [\%]"]

# Plot the table
fig, ax = plt.subplots(figsize=(8, 2))  # Adjust figure size for better readability
ax.axis('tight')
ax.axis('off')

# Create the table
table = ax.table(cellText=data, colLabels=columns, loc='center', cellLoc='center')
table.auto_set_font_size(False)
table.set_fontsize(12)
table.auto_set_column_width(col=list(range(len(columns))))  # Automatically adjust column widths
table.scale(1.5, 1.5)  # Scale width and height of each cell (width=1.5x, height=1.5x)

plt.title("Final Train and Test Accuracies for All Models", pad=10)
plt.show()

"""# Conclusions
As seen in the table above, the use of Batch Normalization improves the already quite high accuracy of the model but the other two regularization techniques do slighly worse.

We can also notice in the graphs that the train accuracy is higher than the test accuracy, as expected.

The generalization error, and also the final Test Accuracy vary slightly between the different regularization techniques.
We may conclude that Batch Normalization is superrior to the different methods, in this very specific setting.

We might expect for even better results by fine tuning some of the regularization techniques' parameters and also by combining some of them together.

Overall, these are solid results.
"""