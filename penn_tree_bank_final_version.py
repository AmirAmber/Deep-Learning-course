# -*- coding: utf-8 -*-
"""Penn Tree Bank -final version

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dQxHWkcxbO3ZWO90TFGI5DpKjWst1Gm2

Imports
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import torch.nn as nn
from torch import optim
import matplotlib.pyplot as plt
from time import time
from google.colab import drive
import os

drive.mount('/content/drive')

# Change path to lab2 directory on your drive
# %cd '/content/drive/My Drive/Shlomi and Roy/Deep Learning/EX 2/ptb'
# %ls
path = os.getcwd()

"""Check GPU availability"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""Loading the Data"""

def load_and_tokenize_data(path):
    # Define file paths
    paths = {
        'train': f'{path}/ptb.train.txt',
        'test': f'{path}/ptb.test.txt',
        'valid': f'{path}/ptb.valid.txt'
    }

    datasets = {}

    # Read files and tokenize
    for split, path in paths.items():
        with open(path, 'r') as f:
            text = f.read()
        tokens = text.strip().split(' ')
        datasets[split] = tokens

    # Create vocabulary and encode data
    vocab = sorted(set(datasets['train']))
    word_to_id = {word: idx for idx, word in enumerate(vocab)}
    for split in datasets:
        datasets[split] = [word_to_id[word] for word in datasets[split]]

    return datasets, len(vocab)


def create_batches(data, batch_size, seq_length):

    data = torch.tensor(data, dtype=torch.long)
    num_batches = data.size(0) // (batch_size * seq_length)
    data = data[:num_batches * batch_size * seq_length].view(batch_size, -1)
    batches = []

    for i in range(0, data.size(1) - seq_length, seq_length):
        x = data[:, i:i + seq_length]
        y = data[:, i + 1:i + seq_length + 1]
        batches.append((x.t(), y.t()))  # Transpose for (seq_length, batch_size)

    return batches

"""Define the models"""

class RNNModel(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, dropout, rnn_type='LSTM', winit=0.05):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn_type = rnn_type

        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.rnn = getattr(nn, rnn_type)(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=False
        )
        self.fc = nn.Linear(hidden_size, vocab_size)

        # Initialize weights
        for param in self.parameters():
            nn.init.uniform_(param, -winit, winit)

    def forward(self, x, hidden):
        x = self.embedding(x)
        output, hidden = self.rnn(x, hidden)
        logits = self.fc(output)
        return logits, hidden

    def init_hidden(self, batch_size):
        if self.rnn_type == 'LSTM':
            return (
                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),
                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)
            )
        else:
            return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)

"""Training and Evaluation"""

def calculate_loss_and_perplexity(model, data, criterion, batch_size):
    """Evaluate model performance."""
    model.eval()
    total_loss = 0
    total_tokens = 0
    hidden = model.init_hidden(batch_size)
    with torch.no_grad():
        for x, y in data:
            x, y = x.to(device), y.to(device)
            logits, hidden = model(x, hidden)
            loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))
            total_loss += loss.item() * y.numel()  # Scale loss by number of tokens
            total_tokens += y.numel()
            hidden = tuple(h.detach() for h in hidden) if isinstance(hidden, tuple) else hidden.detach()
    avg_loss = total_loss / total_tokens
    perplexity = np.exp(avg_loss)
    return avg_loss, perplexity


def train_model(model, train_data, valid_data, epochs, lr, seq_length, max_grad_norm, criterion, optimizer):
    """Train the model."""
    train_losses, valid_losses, test_losses = [], [], []

    for epoch in range(epochs):

      model.train()
      total_loss = 0
      hidden = model.init_hidden(batch_size)

      for x, y in train_data:
          x, y = x.to(device), y.to(device)
          optimizer.zero_grad()
          hidden = tuple(h.detach() for h in hidden) if isinstance(hidden, tuple) else hidden.detach()

          logits, hidden = model(x, hidden)
          loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))
          loss.backward()

          nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
          optimizer.step()

          total_loss += loss.item()

      scheduler.step()


      train_loss, train_perp = calculate_loss_and_perplexity(model, train_data, criterion, batch_size)
      valid_loss, valid_perp = calculate_loss_and_perplexity(model, valid_data, criterion, batch_size)
      test_loss, test_perp = calculate_loss_and_perplexity(model, test_data, criterion, batch_size)

      train_losses.append(train_perp)
      valid_losses.append(valid_perp)
      test_losses.append(test_perp)


      print(f"Epoch {epoch + 1}/{epochs}: Train PPL = {train_perp:.2f}, Valid PPL = {valid_perp:.2f}, Test PPL = {test_perp:.2f}")

    return train_losses, valid_losses, test_losses

"""Plots"""

def plot_results(train_losses, test_losses, title):
    """Plot train and validation perplexities."""
    plt.plot(train_losses, label='Train Perplexity')
    plt.plot(valid_losses, label='Test Perplexity')
    plt.xlabel('Epoch')
    plt.ylabel('Perplexity')
    plt.title(title)
    plt.legend()
    plt.show()

"""Main block - Hyperparameters, data loading and preprocessing, training, evaluation, plots"""

# Hyperparameters
hidden_size = 200
num_layers = 2
seq_length = 35
batch_size = 64
max_grad_norm = 50
default_dropout = 0.3

# Load data
datasets, vocab_size = load_and_tokenize_data(path)
train_data = create_batches(datasets['train'], batch_size, seq_length)
valid_data = create_batches(datasets['valid'], batch_size, seq_length)
test_data = create_batches(datasets['test'], batch_size, seq_length)

# Train and evaluate for 4 configurations:
"""Configurations:
1. LSTM without Dropout
2. LSTM with Dropout
3. GRU without Dropout
4. GRU with Dropout
"""

configurations = [
    ('LSTM', 0),
    ('LSTM', default_dropout),
    ('GRU', 0),
    ('GRU', default_dropout-0.1)
]

# Define epochs and learning rate for each configuration
config_hyperparams = {
    ('LSTM', 0): {'epochs': 30, 'learning_rate': 1e-3},
    ('LSTM', default_dropout): {'epochs': 35, 'learning_rate': 1e-3},
    ('GRU', 0): {'epochs': 35, 'learning_rate': 1e-3},
    ('GRU', default_dropout-0.1): {'epochs': 35, 'learning_rate': 1e-3}
}

final_results = []

for params in configurations:
    rnn_type, dropout = params
    hyperparams = config_hyperparams[params]
    epochs = hyperparams['epochs']
    learning_rate = hyperparams['learning_rate']

    # Initialize model
    model = RNNModel(vocab_size, hidden_size, num_layers, dropout, rnn_type).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-13)

    # Train and evaluate
    train_losses, valid_losses, test_losses = train_model(
        model, train_data, valid_data, epochs, learning_rate, seq_length, max_grad_norm, criterion, optimizer
    )
    final_train_loss = train_losses[-1]
    final_valid_loss = valid_losses[-1]
    final_test_loss = test_losses[-1]
    final_results.append((final_train_loss, final_valid_loss, final_test_loss))

    # Plots
    title = f"{rnn_type} {'without' if dropout == 0 else 'with'} dropout"
    plot_results(train_losses, test_losses, title)

    print(f"Configuration: {rnn_type} {'without' if dropout == 0 else 'with'} dropout")
    print(f"Final Train PPL: {final_train_loss:.2f}, Valid PPL: {final_valid_loss:.2f}, Test PPL: {final_test_loss:.2f}")
    print(f"Learning rate = {learning_rate}, Dropout Keep Probability = {1 - dropout}")

"""Final Results Summary"""

models = ['LSTM without Dropout', 'LSTM with Dropout', 'GRU without Dropout', 'GRU with Dropout']

# Extract final train, valid and test perplexity
final_train_perplexity = [result[0] for result in final_results]
final_valid_perplexity = [result[1] for result in final_results]
final_test_perplexity = [result[2] for result in final_results]

# Data for the table
data = []
for model, final_train_perp, final_valid_perp, final_test_perp in zip(models, final_train_perplexity, final_valid_perplexity, final_test_perplexity):
    data.append([model, f"{final_train_perp:.2f}", f"{final_valid_perp:.2f}", f"{final_test_perp:.2f}"])

# Add column headers
columns = ["Model", "Final Train Perplexity", "Final Validation Perplexity", "Final Test Perplexity"]

# Plot the table
fig, ax = plt.subplots(figsize=(8, 2))  # Adjust figure size for better readability
ax.axis('tight')
ax.axis('off')

# Create the table
table = ax.table(cellText=data, colLabels=columns, loc='center', cellLoc='center')
table.auto_set_font_size(False)
table.set_fontsize(12)
table.auto_set_column_width(col=list(range(len(columns))))  # Automatically adjust column widths
table.scale(1.5, 1.5)  # Scale width and height of each cell (width=1.5x, height=1.5x)

plt.title("Final Results of train, validation, and test for All Models", pad=10)
plt.show()

"""**Conclusions regarding the results:**

As seen in the graphs and table above, we conclude that the GRU architecture is stronger and outperforms the LSTM architecture slightly. We also see an improvement in the perplexity when incorporating dropout, which makes the model more robust. It is also important to note the convergence graphs which show how fast the models "learn". The rate at which the model learns vary significantly with the learning rate and number of epochs, but also with the other hyperparameters. This could also explain the difference between the paper results and ours.
"""